{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cormorant \n",
    "---\n",
    "## Summary\n",
    "Everyone thinks they're special.  In fact, I feel that no other person has an adequately similar taste in music as I do.  This is probably false, but it does provide an interesting premise for a ML project.\n",
    "\n",
    "Recommendation systems typically work (or at least used to) by saying Susie likes song A, Betsy likes song A and B, so Susie probably would like song B.  \n",
    "\n",
    "That's not good enough for a special snowflake like myself, who only likes a very certain type of EDM (at least that's what I'm thinking for the onset).\n",
    "\n",
    "What I'm going to do instead is make a classifier of song spectrograms to determine features of songs I enjoy.  From there, I'll make a youtube crawler to bring me new music (hence the name Cormorant).\n",
    "\n",
    "## Plan\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1.  Pretraining\n",
    "    - ImageNet for the base model.\n",
    "    - Training off the Larger Dataset, then transfering weights over.\n",
    "    \n",
    "    \n",
    "2.  Data\n",
    "    - Need to convert Youtube Playlists to spectrograms.\n",
    "    - Train, Test, Validation Sets\n",
    "    \n",
    "    \n",
    "\n",
    "3.  Learning\n",
    "    - This is a very simple BinaryClassifier (Do I like this music or not?)\n",
    "    - Or do I want to do Regression for how MUCH I like the song?\n",
    "    \n",
    "\n",
    "4.  Action\n",
    "    - Youtube crawler, with seed keywords, URLs.\n",
    "    - Some kind of action to determine live learning and updating of the net - will probably end up making minibatches, then updating all at once.\n",
    "    - Those that pass need to be sent up for review by me somehow - adding to Google Music probably.\n",
    "    \n",
    "    \n",
    "    \n",
    "### Key Technologies Explored\n",
    "\n",
    "- Youtube API\n",
    "- Online learning for a neural network\n",
    "- Neural network with a smaller dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 0 : Getting a dataset to train on\n",
    "---\n",
    "Since the Million Song Dataset seems a bit hard to come by in the 9 years since it was published, we're using `https://github.com/mdeff/fma`.   \n",
    "\n",
    "Going to use the large dataset, 30s with 100k or so of classified songs.  \n",
    "\n",
    "Should be easy enough to run the spectrogram on them and classify them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease                  \u001b[0m  \u001b[0m\u001b[33m\n",
      "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [116 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]      \u001b[0m\n",
      "Get:6 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [10.1 kB]3m\n",
      "Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1089 kB]33m\n",
      "Ign:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
      "Get:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
      "Get:11 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [901 kB][33m\n",
      "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
      "Get:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]mm\u001b[33m3m33m\u001b[33m\u001b[33m\n",
      "Ign:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages3m\u001b[33m\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [27.7 kB]\n",
      "Get:17 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [47.5 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1429 kB]\n",
      "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [255 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [132 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1384 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [8432 B]\n",
      "Fetched 5655 kB in 2s (3512 kB/s)[33m[0m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "29 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libpsl5 publicsuffix\n",
      "The following NEW packages will be installed:\n",
      "  libpsl5 publicsuffix wget\n",
      "0 upgraded, 3 newly installed, 0 to remove and 29 not upgraded.\n",
      "Need to get 455 kB of archives.\n",
      "After this operation, 1320 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpsl5 amd64 0.19.1-5build1 [41.8 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 publicsuffix all 20180223.1310-1 [97.6 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 wget amd64 1.19.4-1ubuntu2.2 [316 kB]\n",
      "Fetched 455 kB in 1s (577 kB/s)0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libpsl5:amd64.\n",
      "(Reading database ... 14995 files and directories currently installed.)\n",
      "Preparing to unpack .../libpsl5_0.19.1-5build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libpsl5:amd64 (0.19.1-5build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 13%]\u001b[49m\u001b[39m [#######...................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 19%]\u001b[49m\u001b[39m [##########................................................] \u001b8Selecting previously unselected package publicsuffix.\n",
      "Preparing to unpack .../publicsuffix_20180223.1310-1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 25%]\u001b[49m\u001b[39m [##############............................................] \u001b8Unpacking publicsuffix (20180223.1310-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [##################........................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Selecting previously unselected package wget.\n",
      "Preparing to unpack .../wget_1.19.4-1ubuntu2.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Unpacking wget (1.19.4-1ubuntu2.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up libpsl5:amd64 (0.19.1-5build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 63%]\u001b[49m\u001b[39m [####################################......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 69%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up publicsuffix (20180223.1310-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 81%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up wget (1.19.4-1ubuntu2.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [##################################################........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 94%]\u001b[49m\u001b[39m [######################################################....] \u001b8Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
      "/sbin/ldconfig.real: File /usr/lib/x86_64-linux-gnu/libnvidia-compiler.so.440.48.02 is empty, not checked.\n",
      "/sbin/ldconfig.real: File /usr/lib/x86_64-linux-gnu/libcuda.so.440.48.02 is empty, not checked.\n",
      "/sbin/ldconfig.real: File /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.440.48.02 is empty, not checked.\n",
      "/sbin/ldconfig.real: File /usr/lib/x86_64-linux-gnu/libnvidia-fatbinaryloader.so.440.48.02 is empty, not checked.\n",
      "/sbin/ldconfig.real: File /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.440.48.02 is empty, not checked.\n",
      "/sbin/ldconfig.real: File /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.440.48.02 is empty, not checked.\n",
      "/sbin/ldconfig.real: File /usr/lib/x86_64-linux-gnu/libnvidia-cfg.so.440.48.02 is empty, not checked.\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!apt update && apt install -y wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-16 05:06:32--  https://os.unil.cloud.switch.ch/fma/fma_large.zip\n",
      "Resolving os.unil.cloud.switch.ch (os.unil.cloud.switch.ch)... 86.119.28.16, 2001:620:5ca1:201::214\n",
      "Connecting to os.unil.cloud.switch.ch (os.unil.cloud.switch.ch)|86.119.28.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 100306112191 (93G) [application/zip]\n",
      "Saving to: 'fma_large.zip'\n",
      "\n",
      "fma_large.zip        33%[=====>              ]  31.35G  9.14MB/s    in 59m 9s  \n",
      "\n",
      "2020-09-16 06:05:42 (9.05 MB/s) - Connection closed at byte 33662652544. Retrying.\n",
      "\n",
      "--2020-09-16 06:05:43--  (try: 2)  https://os.unil.cloud.switch.ch/fma/fma_large.zip\n",
      "Connecting to os.unil.cloud.switch.ch (os.unil.cloud.switch.ch)|86.119.28.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 100306112191 (93G), 66643459647 (62G) remaining [application/zip]\n",
      "Saving to: 'fma_large.zip'\n",
      "\n",
      "fma_large.zip        67%[++++++======>       ]  62.85G  9.14MB/s    in 59m 19s \n",
      "\n",
      "2020-09-16 07:05:03 (9.06 MB/s) - Connection closed at byte 67487679040. Retrying.\n",
      "\n",
      "--2020-09-16 07:05:05--  (try: 3)  https://os.unil.cloud.switch.ch/fma/fma_large.zip\n",
      "Connecting to os.unil.cloud.switch.ch (os.unil.cloud.switch.ch)|86.119.28.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 100306112191 (93G), 32818433151 (31G) remaining [application/zip]\n",
      "Saving to: 'fma_large.zip'\n",
      "\n",
      "fma_large.zip        67%[+++++++++++++       ]  63.01G  9.33MB/s    in 17s     \n",
      "\n",
      "2020-09-16 07:05:22 (9.63 MB/s) - Connection closed at byte 67657650304. Retrying.\n",
      "\n",
      "--2020-09-16 07:05:25--  (try: 4)  https://os.unil.cloud.switch.ch/fma/fma_large.zip\n",
      "Connecting to os.unil.cloud.switch.ch (os.unil.cloud.switch.ch)|86.119.28.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 100306112191 (93G), 32648461887 (30G) remaining [application/zip]\n",
      "Saving to: 'fma_large.zip'\n",
      "\n",
      "fma_large.zip        67%[+++++++++++++       ]  63.17G  9.62MB/s    in 16s     \n",
      "\n",
      "2020-09-16 07:05:42 (9.76 MB/s) - Connection closed at byte 67824362848. Retrying.\n",
      "\n",
      "--2020-09-16 07:05:46--  (try: 5)  https://os.unil.cloud.switch.ch/fma/fma_large.zip\n",
      "Connecting to os.unil.cloud.switch.ch (os.unil.cloud.switch.ch)|86.119.28.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 100306112191 (93G), 32481749343 (30G) remaining [application/zip]\n",
      "Saving to: 'fma_large.zip'\n",
      "\n",
      "fma_large.zip        67%[+++++++++++++       ]  63.31G  9.38MB/s    in 15s     \n",
      "\n",
      "2020-09-16 07:06:02 (9.69 MB/s) - Connection closed at byte 67976612032. Retrying.\n",
      "\n",
      "--2020-09-16 07:06:07--  (try: 6)  https://os.unil.cloud.switch.ch/fma/fma_large.zip\n",
      "Connecting to os.unil.cloud.switch.ch (os.unil.cloud.switch.ch)|86.119.28.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 100306112191 (93G), 32329500159 (30G) remaining [application/zip]\n",
      "Saving to: 'fma_large.zip'\n",
      "\n",
      "fma_large.zip       100%[+++++++++++++======>]  93.42G  9.17MB/s    in 56m 10s \n",
      "\n",
      "2020-09-16 08:02:17 (9.15 MB/s) - 'fma_large.zip' saved [100306112191/100306112191]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://os.unil.cloud.switch.ch/fma/fma_large.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worries\n",
    "So I'm a little concerned at this point.  I looked at the Kaggle competition for the Million Song Dataset - the top results were not promising.  Hopefully, since I'm just classifying whether or not I like the song, I can get better results.  If this fails, I'll look back at this moment as the one where the small cloud on the horizon popped up.\n",
    "\n",
    "I've always been a little concerned that the spectrograms couldn't capture enough subtle differences between songs to make a distinction - i.e. that the signal was too faint to detect.  Since it's very hard to hear a picture (outside of synthesia), it's hard to reason intuitively what the spectrogram actually means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Spectrograms\n",
    "---\n",
    "This work is already done in the Cormorant Github repository, so we'll download a copy of that and go from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!apt update && apt -y install git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cormorant'...\n",
      "remote: Enumerating objects: 49, done.\u001b[K\n",
      "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
      "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
      "remote: Total 49 (delta 14), reused 42 (delta 10), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (49/49), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/amunchet/cormorant.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "Suggested packages:\n",
      "  zip\n",
      "The following NEW packages will be installed:\n",
      "  unzip\n",
      "0 upgraded, 1 newly installed, 0 to remove and 50 not upgraded.\n",
      "Need to get 167 kB of archives.\n",
      "After this operation, 558 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 unzip amd64 6.0-21ubuntu1 [167 kB]\n",
      "Fetched 167 kB in 1s (256 kB/s)0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package unzip.\n",
      "(Reading database ... 16385 files and directories currently installed.)\n",
      "Preparing to unpack .../unzip_6.0-21ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking unzip (6.0-21ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up unzip (6.0-21ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [################################################..........] \u001b8Processing triggers for mime-support (3.60ubuntu1) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!apt install -y unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!unzip fma_large.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Convert training data to spectrogram and move to proper folders\n",
    "---\n",
    "Will need to run spectrogram converter on all ~100k files and put them in the proper folders.  \n",
    "\n",
    "Do we want to multithread this?  FFMPEG can use GPU, but I'm not sure the spectrogramming itself can.\n",
    "\n",
    "Probably want to do 4 threads (seemed to work well last time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Thoughts\n",
    "- Our average song length (NCS) is going to be around 3 minutes.  However, I don't want to just train on 30 seconds, when the actual data is going to be around 1-5 minutes.  I think I'm going to randomize and repeat the entries\n",
    "- This is going to take up quite a bit of space.  I better check that it's okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Downloading https://files.pythonhosted.org/packages/c8/89/63171228d5ced148f5ced50305c89e8576ffc695a90b58fe5bb602b910c2/scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9MB)\n",
      "\u001b[K    100% |################################| 25.9MB 83kB/s  eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from scipy)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.5.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.io.wavfile as wav\n",
    "from numpy.lib import stride_tricks\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000002.mp3\r\n",
      "000003.mp3\r\n",
      "000005.mp3\r\n",
      "000010.mp3\r\n",
      "000020.mp3\r\n",
      "000026.mp3\r\n",
      "000030.mp3\r\n",
      "000046.mp3\r\n",
      "000048.mp3\r\n",
      "000134.mp3\r\n",
      "000135.mp3\r\n",
      "000136.mp3\r\n",
      "000137.mp3\r\n",
      "000138.mp3\r\n",
      "000139.mp3\r\n",
      "000140.mp3\r\n",
      "000141.mp3\r\n",
      "000142.mp3\r\n",
      "000144.mp3\r\n",
      "000145.mp3\r\n"
     ]
    }
   ],
   "source": [
    "ls /mnt/sdf/fma_large/000 | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/Cormorant/cormorant/phase_one\n"
     ]
    }
   ],
   "source": [
    "cd phase_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp /mnt/sdf/fma_large/000/000002.mp3 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000002.mp3  \u001b[0m\u001b[01;32mcormorant.py\u001b[0m*  requirements.txt  \u001b[01;34mspider\u001b[0m/\r\n",
      "README.md   \u001b[01;34mgenerator\u001b[0m/     \u001b[01;32mspectrogram.py\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
      "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Hit:5 http://security.ubuntu.com/ubuntu bionic-security InRelease      \u001b[0m    \n",
      "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease                \n",
      "Hit:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
      "Hit:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
      "Reading package lists... Done \u001b[0m                 \u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "All packages are up to date.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "cython is already the newest version (0.26.1-0.4).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt update && apt install -y cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Librosa\n",
    "This requires llvm-10 to be installed.  That is a pain and requires symlinking `llvm-config` in the path.\n",
    "\n",
    "#### Final conclusion: Librosa installation is horrible.  Going to convert all files to wav instead - way easier.  Which is insane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\r\n",
      "import numpy as np\r\n",
      "from matplotlib import pyplot as plt\r\n",
      "import scipy.io.wavfile as wav\r\n",
      "from numpy.lib import stride_tricks\r\n",
      "import os\r\n",
      "\r\n",
      "\"\"\" short time fourier transform of audio signal \"\"\"\r\n",
      "def stft(sig, frameSize, overlapFac=0.5, window=np.hanning):\r\n",
      "    win = window(frameSize)\r\n",
      "    hopSize = int(frameSize - np.floor(overlapFac * frameSize))\r\n",
      "\r\n",
      "    # zeros at beginning (thus center of 1st window should be for sample nr. 0)   \r\n",
      "    samples = np.append(np.zeros(int(np.floor(frameSize/2.0))), sig)    \r\n",
      "    # cols for windowing\r\n",
      "    cols = np.ceil( (len(samples) - frameSize) / float(hopSize)) + 1\r\n",
      "    # zeros at end (thus samples can be fully covered by frames)\r\n",
      "    samples = np.append(samples, np.zeros(frameSize))\r\n",
      "\r\n",
      "    frames = stride_tricks.as_strided(samples, shape=(int(cols), frameSize), strides=(samples.strides[0]*hopSize, samples.strides[0])).copy()\r\n",
      "    frames *= win\r\n",
      "\r\n",
      "    return np.fft.rfft(frames)    \r\n",
      "\r\n",
      "\"\"\" scale frequency axis logarithmically \"\"\"    \r\n",
      "def logscale_spec(spec, sr=44100, factor=20.):\r\n",
      "    timebins, freqbins = np.shape(spec)\r\n",
      "\r\n",
      "    scale = np.linspace(0, 1, freqbins) ** factor\r\n",
      "    scale *= (freqbins-1)/max(scale)\r\n",
      "    scale = np.unique(np.round(scale))\r\n",
      "\r\n",
      "    # create spectrogram with new freq bins\r\n",
      "    newspec = np.complex128(np.zeros([timebins, len(scale)]))\r\n",
      "    for i in range(0, len(scale)):        \r\n",
      "        if i == len(scale)-1:\r\n",
      "            newspec[:,i] = np.sum(spec[:,int(scale[i]):], axis=1)\r\n",
      "        else:        \r\n",
      "            newspec[:,i] = np.sum(spec[:,int(scale[i]):int(scale[i+1])], axis=1)\r\n",
      "\r\n",
      "    # list center freq of bins\r\n",
      "    allfreqs = np.abs(np.fft.fftfreq(freqbins*2, 1./sr)[:freqbins+1])\r\n",
      "    freqs = []\r\n",
      "    for i in range(0, len(scale)):\r\n",
      "        if i == len(scale)-1:\r\n",
      "            freqs += [np.mean(allfreqs[int(scale[i]):])]\r\n",
      "        else:\r\n",
      "            freqs += [np.mean(allfreqs[int(scale[i]):int(scale[i+1])])]\r\n",
      "\r\n",
      "    return newspec, freqs\r\n",
      "\r\n",
      "\"\"\" plot spectrogram\"\"\"\r\n",
      "def plotstft(audiopath, binsize=2**10, plotpath=None, colormap=\"jet\"):\r\n",
      "    samplerate, samples = wav.read(audiopath)\r\n",
      "\r\n",
      "    s = stft(samples, binsize)\r\n",
      "\r\n",
      "    sshow, freq = logscale_spec(s, factor=1.0, sr=samplerate)\r\n",
      "\r\n",
      "    ims = 20.*np.log10(np.abs(sshow)/10e-6) # amplitude to decibel\r\n",
      "\r\n",
      "    timebins, freqbins = np.shape(ims)\r\n",
      "\r\n",
      "    print(\"timebins: \", timebins)\r\n",
      "    print(\"freqbins: \", freqbins)\r\n",
      "\r\n",
      "    plt.figure(figsize=(15, 7.5))\r\n",
      "    plt.imshow(np.transpose(ims), origin=\"lower\", aspect=\"auto\", cmap=colormap, interpolation=\"none\")\r\n",
      "    # plt.colorbar()\r\n",
      "\r\n",
      "    # plt.xlabel(\"time (s)\")\r\n",
      "    # plt.ylabel(\"frequency (hz)\")\r\n",
      "    # plt.xlim([0, timebins-1])\r\n",
      "    # plt.ylim([0, freqbins])\r\n",
      "    \r\n",
      "    plt.axis('off')\r\n",
      "\r\n",
      "    xlocs = np.float32(np.linspace(0, timebins-1, 5))\r\n",
      "    plt.xticks(xlocs, [\"%.02f\" % l for l in ((xlocs*len(samples)/timebins)+(0.5*binsize))/samplerate])\r\n",
      "    ylocs = np.int16(np.round(np.linspace(0, freqbins-1, 10)))\r\n",
      "    plt.yticks(ylocs, [\"%.02f\" % freq[i] for i in ylocs])\r\n",
      "\r\n",
      "    if plotpath:\r\n",
      "        plt.savefig(plotpath, bbox_inches=\"tight\")\r\n",
      "    else:\r\n",
      "        plt.show()\r\n",
      "\r\n",
      "    plt.clf()\r\n",
      "\r\n",
      "    return ims\r\n",
      "\r\n",
      "\"\"\"\r\n",
      "ims = plotstft(\"test.wav\", plotpath=\"test.png\")\r\n",
      "ims = plotstft(\"bad.wav\", plotpath=\"bad.png\")\r\n",
      "ims = plotstft(\"bad2.wav\", plotpath = \"bad2.png\")\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    for item in [x for x in os.listdir(\".\") if \".wav\" in x]:\r\n",
      "        plotstft(item, plotpath=item + \".png\")\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "cat */spectrogram.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2A: Converting mp3 files to wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the way that I did this in OneTrickFan was to use a `ffmpeg` docker with Nvidia runtime.  Several problems have emerged, however.  The primary being that the MP3 compression is very efficient and the WAV is not - meaning my disk usage will increase by about 20x.  \n",
    "\n",
    "This means that I will instead have to in-place generate the spectrograms all in one shot - cleaning up any wav files I leave behind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Step 3: Initial Model training with spectrograms from large Dataset\n",
    "\n",
    "One of the main questions is: which to use - Tensorflow or FastAI?\n",
    "\n",
    "I think I'll still use one of the larger ImageNets for transfer learning, so FastAI.  I should then be able to train on the large dataset, then transfer over to the much smaller actual dataset.\n",
    "\n",
    "I don't really have a need to do an AutoEncoder, since this is pretty well labelled dataset that I'll be transfer learning off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Step 4: Dataset splitting - songs I like/don't\n",
    "\n",
    "## Step 5: Binary Classifier on Songs\n",
    "\n",
    "### Step 5a: Data verification and spot checking\n",
    "\n",
    "## Step 6: Youtube API exploration and crawling\n",
    "- I've done this in spider\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Online Learning/Updating a neural net\n",
    "\n",
    "So what I'm actually thinking of doing is having batches of songs presented, then graded.  After grading, they'll be added to the data collection of spectrograms and then have a new model trained.  Then a new batch of top song prospects will be brought in, and so on, and so forth.\n",
    "\n",
    "Eventually, there should be a large enough dataset to give good results.\n",
    "\n",
    "This isn't exactly \"online\" learning, but it is incremental, automated improvements of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
